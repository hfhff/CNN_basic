{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model Training with CIFAR10 - Dataset Load & Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets as D\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "transform_train = T.Compose([\n",
    "     #T.RandomCrop(32, padding=4),\n",
    "     #T.RandomHorizontalFlip(),\n",
    "    # T.RandomRotation(15),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "batch_size = 128 # 이미지 묶음 당 이미지 개수\n",
    "\n",
    "#train data 정의\n",
    "trainset = D.CIFAR10(root='./data', train=True,\n",
    "                                        download=False, transform=transform_train) \n",
    "\n",
    "# trainloader를 통해 train_data를 load\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "#test data 정의\n",
    "testset = D.CIFAR10(root='./data', train=False,\n",
    "                                       download=False, transform=transform_test)\n",
    "\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained model - VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "vgg16_model = models.vgg16(pretrained=True)\n",
    "vgg16_model.classifier[6] = nn.Linear(in_features=4096, out_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 0.7715975, Val Loss: 0.4513826\n",
      "Epoch [2/200], Train Loss: 0.4316541, Val Loss: 0.3002174\n",
      "Epoch [3/200], Train Loss: 0.2974399, Val Loss: 0.1746967\n",
      "Epoch [4/200], Train Loss: 0.2044250, Val Loss: 0.1185046\n",
      "Epoch [5/200], Train Loss: 0.1476511, Val Loss: 0.0848632\n",
      "Epoch [6/200], Train Loss: 0.1132706, Val Loss: 0.0775799\n",
      "Epoch [7/200], Train Loss: 0.0872473, Val Loss: 0.0417510\n",
      "Epoch [8/200], Train Loss: 0.0729149, Val Loss: 0.0523715\n",
      "Epoch [9/200], Train Loss: 0.0642783, Val Loss: 0.0534196\n",
      "Epoch [10/200], Train Loss: 0.0511497, Val Loss: 0.0628072\n",
      "Epoch [11/200], Train Loss: 0.0514308, Val Loss: 0.0298368\n",
      "Epoch [12/200], Train Loss: 0.0436152, Val Loss: 0.0297743\n",
      "Epoch [13/200], Train Loss: 0.0464945, Val Loss: 0.0244382\n",
      "Epoch [14/200], Train Loss: 0.0394110, Val Loss: 0.0520219\n",
      "Epoch [15/200], Train Loss: 0.0414222, Val Loss: 0.0396762\n",
      "Epoch [16/200], Train Loss: 0.0380782, Val Loss: 0.0251278\n",
      "Epoch [17/200], Train Loss: 0.0361249, Val Loss: 0.0202630\n",
      "Epoch [18/200], Train Loss: 0.0325939, Val Loss: 0.0364352\n",
      "Epoch [19/200], Train Loss: 0.0288633, Val Loss: 0.0165593\n",
      "Epoch [20/200], Train Loss: 0.0253127, Val Loss: 0.0163522\n",
      "Epoch [21/200], Train Loss: 0.0307530, Val Loss: 0.0291356\n",
      "Epoch [22/200], Train Loss: 0.0307896, Val Loss: 0.0266836\n",
      "Epoch [23/200], Train Loss: 0.0305519, Val Loss: 0.0194930\n",
      "Epoch [24/200], Train Loss: 0.0296463, Val Loss: 0.0485459\n",
      "Epoch [25/200], Train Loss: 0.0289733, Val Loss: 0.0476522\n",
      "Epoch [26/200], Train Loss: 0.0282081, Val Loss: 0.0162668\n",
      "Epoch [27/200], Train Loss: 0.0235202, Val Loss: 0.0138717\n",
      "Epoch [28/200], Train Loss: 0.0254580, Val Loss: 0.0293907\n",
      "Epoch [29/200], Train Loss: 0.0234321, Val Loss: 0.0117628\n",
      "Epoch [30/200], Train Loss: 0.0252284, Val Loss: 0.0108723\n",
      "Epoch [31/200], Train Loss: 0.0268307, Val Loss: 0.0212010\n",
      "Epoch [32/200], Train Loss: 0.0240532, Val Loss: 0.0078216\n",
      "Epoch [33/200], Train Loss: 0.0211135, Val Loss: 0.0150217\n",
      "Epoch [34/200], Train Loss: 0.0202589, Val Loss: 0.0151935\n",
      "Epoch [35/200], Train Loss: 0.0245500, Val Loss: 0.0129712\n",
      "Epoch [36/200], Train Loss: 0.0226478, Val Loss: 0.0181062\n",
      "Epoch [37/200], Train Loss: 0.0219212, Val Loss: 0.0126345\n",
      "Epoch [38/200], Train Loss: 0.0169784, Val Loss: 0.0074605\n",
      "Epoch [39/200], Train Loss: 0.0194808, Val Loss: 0.0247589\n",
      "Epoch [40/200], Train Loss: 0.0208775, Val Loss: 0.0090228\n",
      "Epoch [41/200], Train Loss: 0.0158893, Val Loss: 0.0146217\n",
      "Epoch [42/200], Train Loss: 0.0191250, Val Loss: 0.0234713\n",
      "Epoch [43/200], Train Loss: 0.0179289, Val Loss: 0.0111630\n",
      "Epoch [44/200], Train Loss: 0.0186037, Val Loss: 0.0238627\n",
      "Epoch [45/200], Train Loss: 0.0056440, Val Loss: 0.0012553\n",
      "Epoch [46/200], Train Loss: 0.0011532, Val Loss: 0.0006538\n",
      "Epoch [47/200], Train Loss: 0.0009257, Val Loss: 0.0001218\n",
      "Epoch [48/200], Train Loss: 0.0005752, Val Loss: 0.0002240\n",
      "Epoch [49/200], Train Loss: 0.0007041, Val Loss: 0.0001619\n",
      "Epoch [50/200], Train Loss: 0.0006473, Val Loss: 0.0002068\n",
      "Epoch [51/200], Train Loss: 0.0003517, Val Loss: 0.0001171\n",
      "Epoch [52/200], Train Loss: 0.0003521, Val Loss: 0.0005185\n",
      "Epoch [53/200], Train Loss: 0.0001185, Val Loss: 0.0001694\n",
      "Epoch [54/200], Train Loss: 0.0002410, Val Loss: 0.0007918\n",
      "Epoch [55/200], Train Loss: 0.0001857, Val Loss: 0.0000139\n",
      "Epoch [56/200], Train Loss: 0.0000833, Val Loss: 0.0000575\n",
      "Epoch [57/200], Train Loss: 0.0000548, Val Loss: 0.0001694\n",
      "Epoch [58/200], Train Loss: 0.0008497, Val Loss: 0.0003655\n",
      "Epoch [59/200], Train Loss: 0.0004370, Val Loss: 0.0011114\n",
      "Epoch [60/200], Train Loss: 0.0004860, Val Loss: 0.0000492\n",
      "Epoch [61/200], Train Loss: 0.0004814, Val Loss: 0.0018925\n",
      "Epoch [62/200], Train Loss: 0.0004832, Val Loss: 0.0000564\n",
      "Epoch [63/200], Train Loss: 0.0004615, Val Loss: 0.0000396\n",
      "Epoch [64/200], Train Loss: 0.0001075, Val Loss: 0.0005639\n",
      "Epoch [65/200], Train Loss: 0.0001870, Val Loss: 0.0001132\n",
      "Epoch [66/200], Train Loss: 0.0000408, Val Loss: 0.0000576\n",
      "Epoch [67/200], Train Loss: 0.0000325, Val Loss: 0.0000032\n",
      "Epoch [68/200], Train Loss: 0.0002930, Val Loss: 0.0000117\n",
      "Epoch [69/200], Train Loss: 0.0001902, Val Loss: 0.0003308\n",
      "Epoch [70/200], Train Loss: 0.0001721, Val Loss: 0.0000428\n",
      "Epoch [71/200], Train Loss: 0.0000578, Val Loss: 0.0000025\n",
      "Epoch [72/200], Train Loss: 0.0000794, Val Loss: 0.0000844\n",
      "Epoch [73/200], Train Loss: 0.0000072, Val Loss: 0.0000348\n",
      "Epoch [74/200], Train Loss: 0.0000214, Val Loss: 0.0001633\n",
      "Epoch [75/200], Train Loss: 0.0000675, Val Loss: 0.0000017\n",
      "Epoch [76/200], Train Loss: 0.0000407, Val Loss: 0.0000102\n",
      "Epoch [77/200], Train Loss: 0.0002024, Val Loss: 0.0000046\n",
      "Epoch [78/200], Train Loss: 0.0000040, Val Loss: 0.0001493\n",
      "Epoch [79/200], Train Loss: 0.0000083, Val Loss: 0.0000054\n",
      "Epoch [80/200], Train Loss: 0.0000112, Val Loss: 0.0000023\n",
      "Epoch [81/200], Train Loss: 0.0000035, Val Loss: 0.0000027\n",
      "Epoch [82/200], Train Loss: 0.0000437, Val Loss: 0.0000014\n",
      "Epoch [83/200], Train Loss: 0.0000421, Val Loss: 0.0000020\n",
      "Epoch [84/200], Train Loss: 0.0000981, Val Loss: 0.0000019\n",
      "Epoch [85/200], Train Loss: 0.0003030, Val Loss: 0.0000027\n",
      "Epoch [86/200], Train Loss: 0.0000103, Val Loss: 0.0000020\n",
      "Epoch [87/200], Train Loss: 0.0000057, Val Loss: 0.0000174\n",
      "Epoch [88/200], Train Loss: 0.0000050, Val Loss: 0.0000012\n",
      "Epoch [89/200], Train Loss: 0.0000244, Val Loss: 0.0000024\n",
      "Epoch [90/200], Train Loss: 0.0001253, Val Loss: 0.0000008\n",
      "Epoch [91/200], Train Loss: 0.0000264, Val Loss: 0.0000012\n",
      "Epoch [92/200], Train Loss: 0.0000263, Val Loss: 0.0000014\n",
      "Epoch [93/200], Train Loss: 0.0000130, Val Loss: 0.0000020\n",
      "Epoch [94/200], Train Loss: 0.0000074, Val Loss: 0.0000237\n",
      "Epoch [95/200], Train Loss: 0.0001910, Val Loss: 0.0000017\n",
      "Epoch [96/200], Train Loss: 0.0000055, Val Loss: 0.0000016\n",
      "Epoch [97/200], Train Loss: 0.0000575, Val Loss: 0.0000016\n",
      "Epoch [98/200], Train Loss: 0.0000061, Val Loss: 0.0000007\n",
      "Epoch [99/200], Train Loss: 0.0000281, Val Loss: 0.0000039\n",
      "Epoch [100/200], Train Loss: 0.0000046, Val Loss: 0.0001051\n",
      "Epoch [101/200], Train Loss: 0.0000441, Val Loss: 0.0000045\n",
      "Epoch [102/200], Train Loss: 0.0000031, Val Loss: 0.0000072\n",
      "Epoch [103/200], Train Loss: 0.0000398, Val Loss: 0.0000006\n",
      "Epoch [104/200], Train Loss: 0.0000036, Val Loss: 0.0000017\n",
      "Epoch [105/200], Train Loss: 0.0000174, Val Loss: 0.0000133\n",
      "Epoch [106/200], Train Loss: 0.0000258, Val Loss: 0.0000142\n",
      "Epoch [107/200], Train Loss: 0.0000078, Val Loss: 0.0000214\n",
      "Epoch [108/200], Train Loss: 0.0000032, Val Loss: 0.0000066\n",
      "Epoch [109/200], Train Loss: 0.0000086, Val Loss: 0.0000007\n",
      "Epoch [110/200], Train Loss: 0.0000052, Val Loss: 0.0000024\n",
      "Epoch [111/200], Train Loss: 0.0000131, Val Loss: 0.0000006\n",
      "Epoch [112/200], Train Loss: 0.0000886, Val Loss: 0.0000015\n",
      "Epoch [113/200], Train Loss: 0.0003352, Val Loss: 0.0000012\n",
      "Epoch [114/200], Train Loss: 0.0000149, Val Loss: 0.0000031\n",
      "Epoch [115/200], Train Loss: 0.0000291, Val Loss: 0.0002104\n",
      "Epoch [116/200], Train Loss: 0.0000267, Val Loss: 0.0000020\n",
      "Epoch [117/200], Train Loss: 0.0000040, Val Loss: 0.0000013\n",
      "Epoch [118/200], Train Loss: 0.0000036, Val Loss: 0.0000016\n",
      "Epoch [119/200], Train Loss: 0.0000054, Val Loss: 0.0000025\n",
      "Epoch [120/200], Train Loss: 0.0000031, Val Loss: 0.0000007\n",
      "Epoch [121/200], Train Loss: 0.0000033, Val Loss: 0.0000027\n",
      "Epoch [122/200], Train Loss: 0.0000085, Val Loss: 0.0000006\n",
      "Epoch [123/200], Train Loss: 0.0000023, Val Loss: 0.0000036\n",
      "Epoch [124/200], Train Loss: 0.0001795, Val Loss: 0.0000005\n",
      "Epoch [125/200], Train Loss: 0.0000038, Val Loss: 0.0000244\n",
      "Epoch [126/200], Train Loss: 0.0000521, Val Loss: 0.0000009\n",
      "Epoch [127/200], Train Loss: 0.0000186, Val Loss: 0.0000034\n",
      "Epoch [128/200], Train Loss: 0.0000137, Val Loss: 0.0000014\n",
      "Epoch [129/200], Train Loss: 0.0000722, Val Loss: 0.0000011\n",
      "Epoch [130/200], Train Loss: 0.0000037, Val Loss: 0.0000052\n",
      "Epoch [131/200], Train Loss: 0.0000164, Val Loss: 0.0000018\n",
      "Epoch [132/200], Train Loss: 0.0000079, Val Loss: 0.0000013\n",
      "Epoch [133/200], Train Loss: 0.0000278, Val Loss: 0.0000033\n",
      "Epoch [134/200], Train Loss: 0.0000036, Val Loss: 0.0000049\n",
      "Epoch [135/200], Train Loss: 0.0000141, Val Loss: 0.0000715\n",
      "Epoch [136/200], Train Loss: 0.0000477, Val Loss: 0.0000004\n",
      "Epoch [137/200], Train Loss: 0.0000490, Val Loss: 0.0000083\n",
      "Epoch [138/200], Train Loss: 0.0000048, Val Loss: 0.0000007\n",
      "Epoch [139/200], Train Loss: 0.0000054, Val Loss: 0.0000007\n",
      "Epoch [140/200], Train Loss: 0.0003130, Val Loss: 0.0000006\n",
      "Epoch [141/200], Train Loss: 0.0000074, Val Loss: 0.0000032\n",
      "Epoch [142/200], Train Loss: 0.0001464, Val Loss: 0.0000013\n",
      "Epoch [143/200], Train Loss: 0.0000086, Val Loss: 0.0000017\n",
      "Epoch [144/200], Train Loss: 0.0000417, Val Loss: 0.0000020\n",
      "Epoch [145/200], Train Loss: 0.0000370, Val Loss: 0.0000016\n",
      "Epoch [146/200], Train Loss: 0.0000628, Val Loss: 0.0000061\n",
      "Epoch [147/200], Train Loss: 0.0000179, Val Loss: 0.0000035\n",
      "Epoch [148/200], Train Loss: 0.0000327, Val Loss: 0.0000007\n",
      "Epoch [149/200], Train Loss: 0.0000122, Val Loss: 0.0000021\n",
      "Epoch [150/200], Train Loss: 0.0000578, Val Loss: 0.0000014\n",
      "Epoch [151/200], Train Loss: 0.0000045, Val Loss: 0.0000008\n",
      "Epoch [152/200], Train Loss: 0.0001340, Val Loss: 0.0000007\n",
      "Epoch [153/200], Train Loss: 0.0000072, Val Loss: 0.0000340\n",
      "Epoch [154/200], Train Loss: 0.0000039, Val Loss: 0.0000149\n",
      "Epoch [155/200], Train Loss: 0.0000065, Val Loss: 0.0000006\n",
      "Epoch [156/200], Train Loss: 0.0000112, Val Loss: 0.0000033\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import copy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = vgg16_model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "best_model_weights = copy.deepcopy(model.state_dict())\n",
    "early_stop = False\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in trainloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(trainloader)\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.7f}, Val Loss: {val_loss:.7f}')\n",
    "\n",
    "    # Check for improvement\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= patience:\n",
    "        print('Early stopping!')\n",
    "        early_stop = True\n",
    "        break\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "# Load best model weights if early stopped\n",
    "if early_stop or epoch == num_epochs - 1:\n",
    "    model.load_state_dict(best_model_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in testloader:\n",
    "            images, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 89.66 %\n"
     ]
    }
   ],
   "source": [
    "inference(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
